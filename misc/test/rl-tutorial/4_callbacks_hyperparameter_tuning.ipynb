{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/4_callbacks_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8lIXBiHRYb6"
   },
   "source": [
    "# Stable Baselines3 Tutorial - Callbacks and hyperparameter tuning\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
    "\n",
    "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn how to use [Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) which allow to do monitoring, auto saving, model manipulation, progress bars, ...\n",
    "\n",
    "\n",
    "You will also see that finding good hyperparameters is key to success in RL.\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "# %load_ext jupyter_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owKXXp8rRZI7"
   },
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18ivrnsaSWUn"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PytOtL9GdmrE"
   },
   "source": [
    "# The importance of hyperparameter tuning\n",
    "\n",
    "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
    "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n",
    "\n",
    "Here we demonstrate on a toy example the [Soft Actor Critic](https://arxiv.org/abs/1801.01290) algorithm applied in the Pendulum environment. Note the change in performance between the default and \"tuned\" parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5oVvYHwdnYv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "-a0v3fgwe54j",
    "outputId": "52f15317-d898-4aae-cd53-893e928909b3"
   },
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "eval_env = gym.make(\"Ant-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WRR7kmIeqEB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 443       |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.7      |\n",
      "|    critic_loss     | 1.46      |\n",
      "|    ent_coef        | 0.811     |\n",
      "|    ent_coef_loss   | -0.346    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 422       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.3      |\n",
      "|    critic_loss     | 0.961     |\n",
      "|    ent_coef        | 0.645     |\n",
      "|    ent_coef_loss   | -0.661    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 409       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.7      |\n",
      "|    critic_loss     | 0.746     |\n",
      "|    ent_coef        | 0.52      |\n",
      "|    ent_coef_loss   | -0.807    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 410       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 79.2      |\n",
      "|    critic_loss     | 2.42      |\n",
      "|    ent_coef        | 0.429     |\n",
      "|    ent_coef_loss   | -0.639    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 409       |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 104       |\n",
      "|    critic_loss     | 4.12      |\n",
      "|    ent_coef        | 0.364     |\n",
      "|    ent_coef_loss   | -0.667    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 405       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 113       |\n",
      "|    critic_loss     | 3.2       |\n",
      "|    ent_coef        | 0.312     |\n",
      "|    ent_coef_loss   | -0.53     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 404       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 132       |\n",
      "|    critic_loss     | 2.33      |\n",
      "|    ent_coef        | 0.252     |\n",
      "|    ent_coef_loss   | -0.771    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 406       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 138       |\n",
      "|    critic_loss     | 2.58      |\n",
      "|    ent_coef        | 0.204     |\n",
      "|    ent_coef_loss   | -0.467    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 408       |\n",
      "|    time_elapsed    | 17        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 162       |\n",
      "|    critic_loss     | 3.08      |\n",
      "|    ent_coef        | 0.183     |\n",
      "|    ent_coef_loss   | 0.151     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 407       |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 160       |\n",
      "|    critic_loss     | 3.32      |\n",
      "|    ent_coef        | 0.19      |\n",
      "|    ent_coef_loss   | 0.0809    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    verbose=1,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jQbDcbEheqWj",
    "outputId": "4f664eeb-0374-4db0-c29e-1b8cd131b22b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savan/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (1, 27) for Box environment, please use (3,) or (n_env, 3) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_reward, std_reward \u001b[39m=\u001b[39m evaluate_policy(default_model, eval_env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean_reward:\u001b[39m\u001b[39m{\u001b[39;00mmean_reward\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m +/- \u001b[39m\u001b[39m{\u001b[39;00mstd_reward\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[39m=\u001b[39;49mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[39m=\u001b[39;49mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:555\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    542\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/policies.py:346\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 346\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[1;32m    348\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    349\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/policies.py:264\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    260\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(observation)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     \u001b[39m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     vectorized_env \u001b[39m=\u001b[39m is_vectorized_observation(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space)\n\u001b[1;32m    265\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/utils.py:399\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[39min\u001b[39;00m is_vec_obs_func_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    398\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 399\u001b[0m         \u001b[39mreturn\u001b[39;00m is_vec_obs_func(observation, observation_space)\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/utils.py:266\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    267\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Unexpected observation shape \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBox environment, please use \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mor (n_env, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) for the observation shape.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, observation_space\u001b[39m.\u001b[39mshape)))\n\u001b[1;32m    270\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (1, 27) for Box environment, please use (3,) or (n_env, 3) for the observation shape."
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smMdkZnvfL1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.37e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 226       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.4      |\n",
      "|    critic_loss     | 0.271     |\n",
      "|    ent_coef        | 0.812     |\n",
      "|    ent_coef_loss   | -0.341    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 212       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 45.7      |\n",
      "|    critic_loss     | 0.167     |\n",
      "|    ent_coef        | 0.649     |\n",
      "|    ent_coef_loss   | -0.575    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.31e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 208       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 62        |\n",
      "|    critic_loss     | 0.259     |\n",
      "|    ent_coef        | 0.535     |\n",
      "|    ent_coef_loss   | -0.597    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.2e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.1     |\n",
      "|    critic_loss     | 0.531    |\n",
      "|    ent_coef        | 0.456    |\n",
      "|    ent_coef_loss   | -0.333   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -997     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 74.3     |\n",
      "|    critic_loss     | 0.492    |\n",
      "|    ent_coef        | 0.39     |\n",
      "|    ent_coef_loss   | -0.664   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -847     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 204      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 79       |\n",
      "|    critic_loss     | 0.723    |\n",
      "|    ent_coef        | 0.327    |\n",
      "|    ent_coef_loss   | -0.722   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -753     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 204      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.1     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.27     |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -674     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 203      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 79       |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -613     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 203      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 70.8     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | -0.785   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -561     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 203      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.8     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.158    |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=0,\n",
    ").learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DN05_Io8fMAr",
    "outputId": "a009b1ea-17f7-4f6f-b021-35cf7356b2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-142.95 +/- 86.75\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pi9IwxBYVMl8"
   },
   "source": [
    "Exploring hyperparameter tuning is out of the scope (and time schedule) of this tutorial. However, you need to know that we provide tuned hyperparameter in the [rl zoo](https://github.com/DLR-RM/rl-baselines3-zoo) as well as automatic hyperparameter optimization using [Optuna](https://github.com/pfnet/optuna).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irHk8FXdRUnw"
   },
   "source": [
    "# Callbacks\n",
    "\n",
    "\n",
    "Please read the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html). Although Stable-Baselines3 provides you with a callback collection (e.g. for creating checkpoints or for evaluation), we are going to re-implement some so you can get a good understanding of how they work.\n",
    "\n",
    "To build a custom callback, you need to create a class that derives from `BaseCallback`. This will give you access to events (`_on_training_start`, `_on_step()`) and useful variables (like `self.model` for the RL model).\n",
    "\n",
    "`_on_step` returns a boolean value for whether or not the training should continue.\n",
    "\n",
    "Thanks to the access to the models variables, in particular `self.model`, we are able to even change the parameters of the model without halting the training, or changing the model's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE30k2i7kohh"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjRvJ8zBftL3"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqpPtxaCfynB"
   },
   "source": [
    "Here we have a simple callback that can only be called twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ILY0AkFfzPJ"
   },
   "outputs": [],
   "source": [
    "class SimpleCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    a simple callback that can only be called twice\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SimpleCallback, self).__init__(verbose)\n",
    "        self._called = False\n",
    "\n",
    "    def _on_step(self):\n",
    "        if not self._called:\n",
    "            print(\"callback - first call\")\n",
    "            self._called = True\n",
    "            return True  # returns True, training continues.\n",
    "        print(\"callback - second call\")\n",
    "        return False  # returns False, training stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gTXaNLARUnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "callback - first call\n",
      "callback - second call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x175b2dad0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "model.learn(8000, callback=SimpleCallback())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## First example: Auto saving best model\n",
    "In RL, it is quite useful to keep a clean version of a model as you are training, as we can end up with burn-in of a bad policy. This is a typical use case for callback, as they can call the save function of the model, and observe the training over time.\n",
    "\n",
    "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward.\n",
    "This allows us to save the best model while training.\n",
    "\n",
    "Note that this is not the proper way of evaluating an RL agent, you should create an test environment and evaluate the agent performance in the callback (cf `EvalCallback`). For simplicity, we will be using the training reward as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDI3lKTiiKP9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                        print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TuYLBEaRUn0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 60\n",
      "Best mean reward: -inf - Last mean reward per episode: -14.99\n",
      "Saving new best model at 41 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 80\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -17.95\n",
      "Num timesteps: 100\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -17.95\n",
      "Num timesteps: 120\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -17.95\n",
      "Num timesteps: 140\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -23.81\n",
      "Num timesteps: 160\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -23.81\n",
      "Num timesteps: 180\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -26.21\n",
      "Num timesteps: 200\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -23.31\n",
      "Num timesteps: 220\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 240\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 260\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 280\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 300\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 320\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 340\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 360\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 380\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 400\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 420\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 440\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 460\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 480\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 500\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 520\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 540\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 560\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 580\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 600\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 620\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 640\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 660\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 680\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 700\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 720\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 740\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 760\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 780\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 800\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 820\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 840\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 860\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 880\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 900\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 920\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 940\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 960\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 980\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1020\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1040\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1060\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1080\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1100\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1120\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1140\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1160\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1180\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1200\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -22.02\n",
      "Num timesteps: 1220\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -77.07\n",
      "Num timesteps: 1240\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -77.07\n",
      "Num timesteps: 1260\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -67.87\n",
      "Num timesteps: 1280\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -67.87\n",
      "Num timesteps: 1300\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -61.05\n",
      "Num timesteps: 1320\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -55.70\n",
      "Num timesteps: 1340\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -55.70\n",
      "Num timesteps: 1360\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -55.70\n",
      "Num timesteps: 1380\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -52.69\n",
      "Num timesteps: 1400\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -52.69\n",
      "Num timesteps: 1420\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -52.69\n",
      "Num timesteps: 1440\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -47.03\n",
      "Num timesteps: 1460\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -45.35\n",
      "Num timesteps: 1480\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1500\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1520\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1540\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1560\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1580\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1600\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1620\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1640\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1660\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1680\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1700\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1720\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1740\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1760\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1780\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1800\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1820\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1840\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1860\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1880\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1900\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1920\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1940\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1960\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 1980\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2020\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2040\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2060\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2080\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2100\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2120\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2140\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2160\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2180\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2200\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2220\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2240\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2260\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2280\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2300\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2320\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2340\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2360\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2380\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2400\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2420\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2440\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2460\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -42.88\n",
      "Num timesteps: 2480\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -72.93\n",
      "Num timesteps: 2500\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -68.62\n",
      "Num timesteps: 2520\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -68.62\n",
      "Num timesteps: 2540\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -68.62\n",
      "Num timesteps: 2560\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -67.90\n",
      "Num timesteps: 2580\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -67.90\n",
      "Num timesteps: 2600\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -67.90\n",
      "Num timesteps: 2620\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -66.48\n",
      "Num timesteps: 2640\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -64.22\n",
      "Num timesteps: 2660\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -64.22\n",
      "Num timesteps: 2680\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -64.22\n",
      "Num timesteps: 2700\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -64.22\n",
      "Num timesteps: 2720\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -64.22\n",
      "Num timesteps: 2740\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -63.09\n",
      "Num timesteps: 2760\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -63.09\n",
      "Num timesteps: 2780\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -63.84\n",
      "Num timesteps: 2800\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -63.84\n",
      "Num timesteps: 2820\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2840\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2860\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2880\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2900\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2920\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2940\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2960\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 2980\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3020\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3040\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3060\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3080\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3100\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3120\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3140\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3160\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3180\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3200\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3220\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3240\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3260\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3280\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3300\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3320\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3340\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3360\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3380\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3400\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3420\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3440\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3460\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3480\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3500\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3520\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3540\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3560\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3580\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3600\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3620\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3640\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3660\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3680\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3700\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3720\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3740\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3760\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3780\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3800\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -62.86\n",
      "Num timesteps: 3820\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3840\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3860\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3880\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3900\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3920\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3940\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3960\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.29\n",
      "Num timesteps: 3980\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -82.75\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -82.75\n",
      "Num timesteps: 4020\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.32\n",
      "Num timesteps: 4040\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -81.32\n",
      "Num timesteps: 4060\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4080\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4100\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4120\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4140\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4160\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4180\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4200\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4220\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4240\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4260\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4280\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4300\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4320\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4340\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4360\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4380\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4400\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4420\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4440\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4460\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4480\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4500\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4520\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4540\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4560\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4580\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4600\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4620\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4640\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4660\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4680\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4700\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4720\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4740\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4760\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4780\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4800\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4820\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4840\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4860\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4880\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4900\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4920\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4940\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4960\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 4980\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -14.99 - Last mean reward per episode: -78.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x16af98150>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1, monitor_dir=log_dir)\n",
    "env = make_vec_env(\"Ant-v4\", n_envs=1, monitor_dir=log_dir)\n",
    "# it is equivalent to:\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = Monitor(env, log_dir)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx18FkEORUn3"
   },
   "source": [
    "## Second example: Realtime plotting of performance\n",
    "While training, it is sometimes useful to how the training progresses over time, relative to the episodic reward.\n",
    "For this, Stable-Baselines has [Tensorboard support](https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html), however this can be very combersome, especially in disk space usage. \n",
    "\n",
    "**NOTE: Unfortunately live plotting does not work out of the box on google colab**\n",
    "\n",
    "Here, we can use callback again, to plot the episodic reward in realtime, using the monitoring wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0Bu1HWKRUn4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m plotting_callback \u001b[39m=\u001b[39m PlottingCallback()\n\u001b[1;32m     43\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39m10000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mplotting_callback)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    183\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 184\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mPlottingCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mset_xlim([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocals[\u001b[39m\"\u001b[39m\u001b[39mtotal_timesteps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.02\u001b[39m, \n\u001b[1;32m     30\u001b[0m                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocals[\u001b[39m\"\u001b[39m\u001b[39mtotal_timesteps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m1.02\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mautoscale_view(\u001b[39mTrue\u001b[39;00m,\u001b[39mTrue\u001b[39;00m,\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mdraw()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    401\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/figure.py:3140\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3140\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3141\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3143\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3144\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[39mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3065\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3067\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/axis.py:1377\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m renderer\u001b[39m.\u001b[39mopen_group(\u001b[39m__name__\u001b[39m, gid\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_gid())\n\u001b[1;32m   1376\u001b[0m ticks_to_draw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1377\u001b[0m tlb1, tlb2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   1379\u001b[0m \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m   1380\u001b[0m     tick\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/axis.py:1304\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39mif\u001b[39;00m renderer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     renderer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mreturn\u001b[39;00m ([tick\u001b[39m.\u001b[39;49mlabel1\u001b[39m.\u001b[39;49mget_window_extent(renderer)\n\u001b[1;32m   1305\u001b[0m          \u001b[39mfor\u001b[39;49;00m tick \u001b[39min\u001b[39;49;00m ticks \u001b[39mif\u001b[39;49;00m tick\u001b[39m.\u001b[39;49mlabel1\u001b[39m.\u001b[39;49mget_visible()],\n\u001b[1;32m   1306\u001b[0m         [tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1307\u001b[0m          \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/axis.py:1304\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39mif\u001b[39;00m renderer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     renderer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mreturn\u001b[39;00m ([tick\u001b[39m.\u001b[39;49mlabel1\u001b[39m.\u001b[39;49mget_window_extent(renderer)\n\u001b[1;32m   1305\u001b[0m          \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel1\u001b[39m.\u001b[39mget_visible()],\n\u001b[1;32m   1306\u001b[0m         [tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1307\u001b[0m          \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/text.py:962\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    960\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_unitless_position()\n\u001b[1;32m    961\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_transform()\u001b[39m.\u001b[39mtransform((x, y))\n\u001b[0;32m--> 962\u001b[0m bbox \u001b[39m=\u001b[39m bbox\u001b[39m.\u001b[39;49mtranslated(x, y)\n\u001b[1;32m    963\u001b[0m \u001b[39mreturn\u001b[39;00m bbox\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/transforms.py:626\u001b[0m, in \u001b[0;36mBboxBase.translated\u001b[0;34m(self, tx, ty)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslated\u001b[39m(\u001b[39mself\u001b[39m, tx, ty):\n\u001b[1;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct a `Bbox` by translating this one by *tx* and *ty*.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mreturn\u001b[39;00m Bbox(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_points \u001b[39m+\u001b[39;49m (tx, ty))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/matplotlib/transforms.py:767\u001b[0m, in \u001b[0;36mBbox.__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mBbox points must be of the form \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    765\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[[x0, y0], [x1, y1]]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    766\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_points \u001b[39m=\u001b[39m points\n\u001b[0;32m--> 767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minpos \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf])\n\u001b[1;32m    768\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39m# it is helpful in some contexts to know if the bbox is a\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39m# default or has been mutated; we store the orig points to\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39m# support the mutated methods\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the performance in realtime.\n",
    "\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get the monitor's data\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if self._plot is None: # make the plot\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(6,3))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, = ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.show()\n",
    "        else: # update and rescale the plot\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "                                    self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True,True,True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "        \n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('MountainCarContinuous-v0', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "plotting_callback = PlottingCallback()\n",
    "        \n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(10000, callback=plotting_callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49RVX7ieRUn7"
   },
   "source": [
    "## Third example: Progress bar\n",
    "Quality of life improvement are always welcome when developping and using RL. Here, we used [tqdm](https://tqdm.github.io/) to show a progress bar of the training, along with number of timesteps per second and the estimated time remaining to the end of the training:\n",
    "\n",
    "Please note that this callback is already included in SB3 and can be used by passing `progress_bar=True` to the `learn()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXa8f6FsRUn8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savan/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|| 2000/2000 [00:06<00:00, 312.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pbar):\n",
    "        super().__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps):  # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def __enter__(self):  # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "\n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "model = TD3(\"MlpPolicy\", \"Pendulum-v1\", verbose=0)\n",
    "# Using a context manager garanties that the tqdm progress bar closes correctly\n",
    "with ProgressBarManager(2000) as callback:\n",
    "    model.learn(2000, callback=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBF4ij46RUoC"
   },
   "source": [
    "## Forth example: Composition\n",
    "Thanks to the functional nature of callbacks, it is possible to do a composition of callbacks, into a single callback. This means we can auto save our best model, show the progress bar and episodic reward of the training.\n",
    "\n",
    "The callbacks are automatically composed when you pass a list to the `learn()` method. Under the hood, a `CallbackList` is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hU3T9tkRUoD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1416it [00:00, 2899.51it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -1124.37\n",
      "Saving new best model at 1000 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [00:00, 2861.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2000\n",
      "Best mean reward: -1124.37 - Last mean reward per episode: -274.41\n",
      "Saving new best model at 1398 timesteps\n",
      "Saving new best model to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:01<00:00, 938.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
    "env = make_vec_env('Ant-v4', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "# Create callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "with ProgressBarManager(1000) as progress_callback:\n",
    "  # This is equivalent to callback=CallbackList([progress_callback, auto_save_callback])\n",
    "  model.learn(1000, callback=[progress_callback, auto_save_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRB4-qIxg_c9"
   },
   "source": [
    "## Exercise: Code your own callback\n",
    "\n",
    "\n",
    "The previous examples showed the basics of what is a callback and what you do with it.\n",
    "\n",
    "The goal of this exercise is to create a callback that will evaluate the model using a test environment and save it if this is the best known model.\n",
    "\n",
    "To make things easier, we are going to use a class instead of a function with the magic method `__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOn0Sr3OhC2U"
   },
   "outputs": [],
   "source": [
    "class EvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "\n",
    "    :param eval_env: (gym.Env) The environment used for initialization\n",
    "    :param n_eval_episodes: (int) The number of episodes to test the agent\n",
    "    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        This method will be called by the model.\n",
    "\n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n_calls is automatically updated because\n",
    "        # we derive from BaseCallback\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # === YOUR CODE HERE ===#\n",
    "            # Evaluate the agent:\n",
    "            # you need to do self.n_eval_episodes loop using self.eval_env\n",
    "            # hint: you can use self.model.predict(obs, deterministic=True)\n",
    "\n",
    "            # Save the agent if needed\n",
    "            # and update self.best_mean_reward\n",
    "\n",
    "            print(\"Best mean reward: {:.2f}\".format(self.best_mean_reward))\n",
    "\n",
    "            # ====================== #\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO0I81jAkQ0z"
   },
   "source": [
    "### Test your callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OMop3TlkTbx"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# ====================== #\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# Train the RL model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39mint\u001b[39m(\u001b[39m100000\u001b[39m), callback\u001b[39m=\u001b[39mcallback)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'learn'"
     ]
    }
   ],
   "source": [
    "# Env used for training\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# Env for evaluating the agent\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# === YOUR CODE HERE ===#\n",
    "# Create the callback object\n",
    "callback = None\n",
    "\n",
    "# Create the RL model\n",
    "model = None\n",
    "\n",
    "# ====================== #\n",
    "\n",
    "# Train the RL model\n",
    "model.learn(int(100000), callback=callback)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wS20a_NfMAh"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "In this notebook we have seen:\n",
    "- that good hyperparameters are key to the success of RL, you should not except the default ones to work on every problems\n",
    "- what is a callback and what you can do with it\n",
    "- how to create your own callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA4gCDtogIaD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "4_callbacks_hyperparameter_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
