{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Stable Baselines3 Tutorial - Getting Started\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
    "\n",
    "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
    "\n",
    "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn the basics for using stable baselines library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "# %load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "colab_type": "code",
    "id": "gWskDE2c9WoN",
    "outputId": "03477445-4249-49c3-ddba-4e12df09e98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning:\u001b[0m No available formula with the name \"xvfb\".\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSearching for similarly named formulae and casks...\u001b[0m\n",
      "\u001b[31mError:\u001b[0m No formulae or casks found for xvfb.\n"
     ]
    }
   ],
   "source": [
    "!brew install ffmpeg freeglut xvfb  # For visualization\n",
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcX8hEcaUpR0"
   },
   "source": [
    "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
    "You can find a list of available environment [here](https://gymnasium.farama.org/environments/classic_control/).\n",
    "\n",
    "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIedd7Pz9sOs"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ae32CtgzTG3R"
   },
   "source": [
    "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7tKaBFrTR0a"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0_8OQbOTTNT"
   },
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROUJr675TT01"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RapkYvTXL7Cd"
   },
   "source": [
    "## Create the Gym env and instantiate the agent\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
    "\n",
    "\n",
    "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
    "\n",
    "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
    "\n",
    "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
    "\n",
    "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
    "\n",
    "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
    "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUWGZp3i9wyf"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4efFdrQ7MBvl"
   },
   "source": [
    "We create a helper function to evaluate the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63M8mSKR-6Zt"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseAlgorithm,\n",
    "    num_episodes: int = 100,\n",
    "    deterministic: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `num_episodes`.\n",
    "\n",
    "    :param model: the RL Agent\n",
    "    :param env: the gym Environment\n",
    "    :param num_episodes: number of episodes to evaluate it\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward for the last `num_episodes`\n",
    "    \"\"\"\n",
    "    # This function will only work for a single environment\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        # Note: SB3 VecEnv resets automatically:\n",
    "        # https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\n",
    "        # obs = vec_env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            # `deterministic` is to use deterministic actions\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, _info = vec_env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(f\"Mean reward: {mean_episode_reward:.2f} - Num episodes: {num_episodes}\")\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjEVOIY8NVeK"
   },
   "source": [
    "Let's evaluate the un-trained agent, this should be a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xDHLMA6NFk95",
    "outputId": "231b2170-a607-48ed-e9d9-daef596f6384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 24.24 - Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_episodes=100, deterministic=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjjPxrwkYJ2i"
   },
   "source": [
    "Stable-Baselines already provides you with that helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8z6K9YImYJEx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oPTHjxyZSOL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 23.99 +/- 10.92\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5UoXTZPNdFE"
   },
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4cfSXIB-pTF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1689c3010>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygl_gVmV_QP7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savan/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:445.11 +/- 93.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A00W6yY3NkHG"
   },
   "source": [
    "Apparently the training went well, the mean reward increased a lot ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVm9QPNVwKXN"
   },
   "source": [
    "### Prepare video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPyfQxD5z26J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: Xvfb: command not found\n"
     ]
    }
   ],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLzXxO8VMD6N"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTRNUfulOGaF"
   },
   "source": [
    "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trag9dQpOIhx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param env_id: (str)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOObbeu5MMlR"
   },
   "source": [
    "### Visualize trained agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iATu7AiyMQW2",
    "outputId": "68acb027-6c94-4389-8456-2cfb11494814"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 1/501 [02:46<23:10:13, 166.83s/it, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to /Users/savan/Documents/UofG/MSc Project/Code/rl-tutorial-jnrr19-sb3/videos/ppo-cartpole-step-0-to-step-500.mp4\n",
      "Moviepy - Building video /Users/savan/Documents/UofG/MSc Project/Code/rl-tutorial-jnrr19-sb3/videos/ppo-cartpole-step-0-to-step-500.mp4.\n",
      "Moviepy - Writing video /Users/savan/Documents/UofG/MSc Project/Code/rl-tutorial-jnrr19-sb3/videos/ppo-cartpole-step-0-to-step-500.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file /Users/savan/Documents/UofG/MSc Project/Code/rl-tutorial-jnrr19-sb3/videos/ppo-cartpole-step-0-to-step-500.mp4:\n\n b\"Unrecognized option 'preset'.\\nError splitting the argument list: Option not found\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:136\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m PY3:\n\u001b[0;32m--> 136\u001b[0m    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproc\u001b[39m.\u001b[39;49mstdin\u001b[39m.\u001b[39;49mwrite(img_array\u001b[39m.\u001b[39;49mtobytes())\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m record_video(\u001b[39m\"\u001b[39;49m\u001b[39mCartPole-v1\u001b[39;49m\u001b[39m\"\u001b[39;49m, model, video_length\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mppo-cartpole\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(video_length):\n\u001b[1;32m     24\u001b[0m     action, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[0;32m---> 25\u001b[0m     obs, _, _, _ \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Close the video recorder\u001b[39;00m\n\u001b[1;32m     28\u001b[0m eval_env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:188\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:96\u001b[0m, in \u001b[0;36mVecVideoRecorder.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorded_frames \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_length:\n\u001b[1;32m     95\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving video to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_recorder\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose_video_recorder()\n\u001b[1;32m     97\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_video_enabled():\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_video_recorder()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:104\u001b[0m, in \u001b[0;36mVecVideoRecorder.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose_video_recorder\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecording:\n\u001b[0;32m--> 104\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvideo_recorder\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecording \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorded_frames \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:161\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     clip \u001b[39m=\u001b[39m ImageSequenceClip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorded_frames, fps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes_per_sec)\n\u001b[1;32m    160\u001b[0m     moviepy_logger \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_logger \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 161\u001b[0m     clip\u001b[39m.\u001b[39;49mwrite_videofile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, logger\u001b[39m=\u001b[39;49mmoviepy_logger)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[39m# No frames captured. Set metadata.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<decorator-gen-69>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m\u001b[39m not set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[0;32m<decorator-gen-68>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    130\u001b[0m new_a \u001b[39m=\u001b[39m [fun(arg) \u001b[39mif\u001b[39;00m (name\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m arg\n\u001b[1;32m    131\u001b[0m          \u001b[39mfor\u001b[39;00m (arg, name) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(a, names)]\n\u001b[1;32m    132\u001b[0m new_kw \u001b[39m=\u001b[39m {k: fun(v) \u001b[39mif\u001b[39;00m k\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m v\n\u001b[1;32m    133\u001b[0m          \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m k\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49mnew_a, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kw)\n",
      "File \u001b[0;32m<decorator-gen-67>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m clip\u001b[39m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[39m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[39m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[39m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[39mself\u001b[39;49m, filename, fps, codec,\n\u001b[1;32m    301\u001b[0m                    bitrate\u001b[39m=\u001b[39;49mbitrate,\n\u001b[1;32m    302\u001b[0m                    preset\u001b[39m=\u001b[39;49mpreset,\n\u001b[1;32m    303\u001b[0m                    write_logfile\u001b[39m=\u001b[39;49mwrite_logfile,\n\u001b[1;32m    304\u001b[0m                    audiofile\u001b[39m=\u001b[39;49maudiofile,\n\u001b[1;32m    305\u001b[0m                    verbose\u001b[39m=\u001b[39;49mverbose, threads\u001b[39m=\u001b[39;49mthreads,\n\u001b[1;32m    306\u001b[0m                    ffmpeg_params\u001b[39m=\u001b[39;49mffmpeg_params,\n\u001b[1;32m    307\u001b[0m                    logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m remove_temp \u001b[39mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:228\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m             frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdstack([frame,mask])\n\u001b[0;32m--> 228\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_frame(frame)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m write_logfile:\n\u001b[1;32m    231\u001b[0m     logfile\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:180\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid encoder type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m ffmpeg_error:\n\u001b[1;32m    176\u001b[0m     error \u001b[39m=\u001b[39m error \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mThe video export failed because the codec \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mor file extension you provided is not a video\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(error)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file /Users/savan/Documents/UofG/MSc Project/Code/rl-tutorial-jnrr19-sb3/videos/ppo-cartpole-step-0-to-step-500.mp4:\n\n b\"Unrecognized option 'preset'.\\nError splitting the argument list: Option not found\\n\""
     ]
    }
   ],
   "source": [
    "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n4i-fW3NojZ"
   },
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y8zg4V566qD"
   },
   "source": [
    "## Bonus: Train a RL Model in One Line\n",
    "\n",
    "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaOPfOrwWEP4"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrI6f5fWnzp-"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we have seen:\n",
    "- how to define and train a RL model using stable baselines3, it takes only one line of code ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73ji3gbNDkf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52.89\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -52.89 - Last mean reward per episode: 446.34\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -335.50\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -666.75\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -706.12\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -577.29\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -461.60\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -347.89\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -252.84\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -180.06\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -115.64\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -65.07\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: -10.39\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 36.33\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 83.76\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 124.87\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 160.76\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 194.78\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 226.17\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 255.17\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 284.51\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 313.45\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 338.48\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 362.41\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 385.59\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 412.78\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 434.24\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 446.34 - Last mean reward per episode: 459.11\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 459.11 - Last mean reward per episode: 477.81\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 477.81 - Last mean reward per episode: 505.65\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 505.65 - Last mean reward per episode: 512.70\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 512.70 - Last mean reward per episode: 518.91\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 518.91 - Last mean reward per episode: 537.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 537.97 - Last mean reward per episode: 551.58\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 551.58 - Last mean reward per episode: 550.08\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 551.58 - Last mean reward per episode: 562.92\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 562.92 - Last mean reward per episode: 584.24\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 584.24 - Last mean reward per episode: 606.33\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 606.33 - Last mean reward per episode: 614.55\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 614.55 - Last mean reward per episode: 626.78\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 626.78 - Last mean reward per episode: 652.53\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 652.53 - Last mean reward per episode: 633.47\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 652.53 - Last mean reward per episode: 645.20\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 652.53 - Last mean reward per episode: 658.94\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 658.94 - Last mean reward per episode: 676.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 676.79 - Last mean reward per episode: 678.48\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 678.48 - Last mean reward per episode: 695.36\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 695.36 - Last mean reward per episode: 716.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 716.35 - Last mean reward per episode: 705.76\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 716.35 - Last mean reward per episode: 724.07\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 724.07 - Last mean reward per episode: 742.54\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 742.54 - Last mean reward per episode: 760.83\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 760.83 - Last mean reward per episode: 751.73\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 760.83 - Last mean reward per episode: 765.28\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 765.28 - Last mean reward per episode: 774.20\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 774.20 - Last mean reward per episode: 791.77\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 791.77 - Last mean reward per episode: 808.74\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 808.74 - Last mean reward per episode: 822.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 822.35 - Last mean reward per episode: 836.75\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 836.75 - Last mean reward per episode: 852.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 852.97 - Last mean reward per episode: 868.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 868.72 - Last mean reward per episode: 883.65\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 883.65 - Last mean reward per episode: 899.23\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 899.23 - Last mean reward per episode: 913.48\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 913.48 - Last mean reward per episode: 926.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 926.62 - Last mean reward per episode: 942.43\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 942.43 - Last mean reward per episode: 938.68\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 942.43 - Last mean reward per episode: 952.09\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 952.09 - Last mean reward per episode: 964.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 964.79 - Last mean reward per episode: 977.75\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 977.75 - Last mean reward per episode: 991.16\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 991.16 - Last mean reward per episode: 1004.15\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 1004.15 - Last mean reward per episode: 1016.40\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 1016.40 - Last mean reward per episode: 1029.19\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 1029.19 - Last mean reward per episode: 1041.31\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 1041.31 - Last mean reward per episode: 1053.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 1053.72 - Last mean reward per episode: 1064.84\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 1064.84 - Last mean reward per episode: 1053.12\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 1064.84 - Last mean reward per episode: 1064.13\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 1064.84 - Last mean reward per episode: 1074.03\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 1074.03 - Last mean reward per episode: 1084.81\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 1084.81 - Last mean reward per episode: 1075.77\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 1084.81 - Last mean reward per episode: 1086.33\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 1086.33 - Last mean reward per episode: 1083.79\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 1086.33 - Last mean reward per episode: 1088.63\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 1088.63 - Last mean reward per episode: 1099.01\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 1099.01 - Last mean reward per episode: 1113.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 1113.72 - Last mean reward per episode: 1123.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 1123.72 - Last mean reward per episode: 1123.20\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 1123.72 - Last mean reward per episode: 1134.47\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 1134.47 - Last mean reward per episode: 1174.67\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 1174.67 - Last mean reward per episode: 1220.20\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 1220.20 - Last mean reward per episode: 1241.76\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 1241.76 - Last mean reward per episode: 1240.08\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 1241.76 - Last mean reward per episode: 1241.51\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 1241.76 - Last mean reward per episode: 1236.92\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 1241.76 - Last mean reward per episode: 1251.56\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 1251.56 - Last mean reward per episode: 1265.60\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 1265.60 - Last mean reward per episode: 1278.37\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 1278.37 - Last mean reward per episode: 1293.20\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 1293.20 - Last mean reward per episode: 1308.20\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 1308.20 - Last mean reward per episode: 1323.16\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 1323.16 - Last mean reward per episode: 1336.39\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 1336.39 - Last mean reward per episode: 1341.91\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 1341.91 - Last mean reward per episode: 1354.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 1354.99 - Last mean reward per episode: 1367.87\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 1367.87 - Last mean reward per episode: 1379.52\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 1379.52 - Last mean reward per episode: 1391.26\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 1391.26 - Last mean reward per episode: 1404.84\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 1404.84 - Last mean reward per episode: 1418.77\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 1418.77 - Last mean reward per episode: 1430.30\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 1430.30 - Last mean reward per episode: 1442.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 1442.44 - Last mean reward per episode: 1460.01\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 1460.01 - Last mean reward per episode: 1472.24\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 1472.24 - Last mean reward per episode: 1480.04\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 1480.04 - Last mean reward per episode: 1481.96\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 1481.96 - Last mean reward per episode: 1489.59\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 1489.59 - Last mean reward per episode: 1501.53\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 1501.53 - Last mean reward per episode: 1509.76\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 1509.76 - Last mean reward per episode: 1525.67\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 1525.67 - Last mean reward per episode: 1540.10\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 1540.10 - Last mean reward per episode: 1550.49\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 1550.49 - Last mean reward per episode: 1548.87\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 1550.49 - Last mean reward per episode: 1564.47\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 1564.47 - Last mean reward per episode: 1582.90\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 1582.90 - Last mean reward per episode: 1594.64\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 1594.64 - Last mean reward per episode: 1591.12\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 1594.64 - Last mean reward per episode: 1599.61\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 1599.61 - Last mean reward per episode: 1622.63\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 1622.63 - Last mean reward per episode: 1631.86\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 1631.86 - Last mean reward per episode: 1636.74\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 1636.74 - Last mean reward per episode: 1663.32\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 1663.32 - Last mean reward per episode: 1682.85\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 1682.85 - Last mean reward per episode: 1694.11\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 1694.11 - Last mean reward per episode: 1702.43\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 1702.43 - Last mean reward per episode: 1695.97\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 1702.43 - Last mean reward per episode: 1711.86\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 1711.86 - Last mean reward per episode: 1718.19\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 1718.19 - Last mean reward per episode: 1720.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 1720.35 - Last mean reward per episode: 1737.01\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 1737.01 - Last mean reward per episode: 1757.29\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 1757.29 - Last mean reward per episode: 1762.22\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 1762.22 - Last mean reward per episode: 1766.85\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 1766.85 - Last mean reward per episode: 1771.34\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 1771.34 - Last mean reward per episode: 1791.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 1791.35 - Last mean reward per episode: 1798.80\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 1798.80 - Last mean reward per episode: 1793.72\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 1798.80 - Last mean reward per episode: 1798.23\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 1798.80 - Last mean reward per episode: 1804.61\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1796.03\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1799.85\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1769.30\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1746.04\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1748.87\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1763.15\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1733.73\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1749.60\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1754.05\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1758.28\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1762.62\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1764.95\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1767.79\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1771.73\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1775.03\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1777.88\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1779.23\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1792.19\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1795.01\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 1804.61 - Last mean reward per episode: 1817.50\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 1817.50 - Last mean reward per episode: 1820.26\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 1820.26 - Last mean reward per episode: 1809.33\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 1820.26 - Last mean reward per episode: 1811.72\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 1820.26 - Last mean reward per episode: 1828.57\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 1828.57 - Last mean reward per episode: 1830.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 1830.35 - Last mean reward per episode: 1845.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 1845.97 - Last mean reward per episode: 1854.23\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 1854.23 - Last mean reward per episode: 1856.92\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 1856.92 - Last mean reward per episode: 1859.73\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 1859.73 - Last mean reward per episode: 1867.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 1867.44 - Last mean reward per episode: 1869.76\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 1869.76 - Last mean reward per episode: 1883.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 1883.99 - Last mean reward per episode: 1905.33\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 1905.33 - Last mean reward per episode: 1893.74\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 1905.33 - Last mean reward per episode: 1899.44\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 1905.33 - Last mean reward per episode: 1903.04\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 1905.33 - Last mean reward per episode: 1927.06\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 1927.06 - Last mean reward per episode: 1958.96\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 1958.96 - Last mean reward per episode: 1988.89\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 1988.89 - Last mean reward per episode: 1990.57\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 1990.57 - Last mean reward per episode: 1992.76\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 1992.76 - Last mean reward per episode: 1996.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 1996.99 - Last mean reward per episode: 1999.49\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 1999.49 - Last mean reward per episode: 2001.43\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 2001.43 - Last mean reward per episode: 2004.00\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 2004.00 - Last mean reward per episode: 2005.17\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 2005.17 - Last mean reward per episode: 2015.42\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 2015.42 - Last mean reward per episode: 2017.57\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 2017.57 - Last mean reward per episode: 2019.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 2019.79 - Last mean reward per episode: 2024.80\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 2024.80 - Last mean reward per episode: 2028.11\n",
      "Saving new best model to tmp/best_model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "env = gym.make(\"Ant-v4\")\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Add some action noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "# Because we use parameter noise, we should use a MlpPolicy with layer normalization\n",
    "model = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=0)\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "# Train the agent\n",
    "timesteps = 2e5\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"TD3 Ant\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savan/.pyenv/versions/3.11.4/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:215: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (None) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# model = TD3.load(\"sac_ant\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_getting_started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
